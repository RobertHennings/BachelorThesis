#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Nov 18 16:49:52 2022

@author: Robert_Hennings
"""

#This script accompanies the bachelor thesis project of Robert Hennings
#It serves the loading of the single equity securities data timeseries


"""
The class consits out of functionalities:
1) For a list of Reuters Eikon Security tickers the asked for timeseries will be downloaded and saved in a provided folder
2) For a provided filepath all .csv files will be read in and stored inot one giant masetr dataframe by concatenating all columns accordingly to the timespan

"""
class LoadEikonData():
    def __init__(self,api_key):
        #The API key that can be generated by the EIKON Application
        self.api_key = api_key
        
        
        
    def LoadFromTickerList(self,ticker_list, path_save, start_date, end_date, frequency):
        """
        

        Parameters
        ----------
        ticker_list : TYPE list
            The Eikon RIC Ticker list, for which historical time series should be loaded and saved.
        path_save : TYPE string
            The file path wher the single timeseries files should be saved in.
        start_date : TYPE string
            The starting date from which on to load the historical timeseries from Eikon.
        end_date : TYPE string
            The end date from which to load the historical timeseries from Eikon.
        frequency : TYPE string
            The data frequency to load, "d" for daily series.

        Returns
        -------
        Single Timeseries files named: <RIC_Ticker>_Timeseries.csv saved in the provided file path.

        """
        import eikon as ek
        import pandas as pd
        #set api ket to use
        ek.set_app_key(self.api_key)
        #adding an error list in which we will save tickers where we got an error loading the data
        error_list = []
        
        
        #apply a try except loop for loading
        #it will save for each ticker its own .csv timeseries file in the path
        for ticker in ticker_list[0:len(ticker_list)]:
            try:
                #process = round()
                df = ek.get_timeseries([ticker], start_date = start_date,
                                                 end_date = end_date,
                                                 interval = frequency)
                df.to_csv(path_save+"//"+ticker+"_"+"Timeseries.csv", index=True)
            except:
                error_list.append(ticker)
                print("Difficulties with loading: ",ticker)
    
        #The result will be loaded .csv files in the provided path
        #Next we want to load only specific columns from the single .csv files
        #These columns should be saved in one big Master Dataframe lined up by the correct time
        
        
        
        
    def CreateMasterDF(path_read, start_date, end_date, amount_to_load):
        """
        

        Parameters
        ----------
        path_read : TYPE string
            The path where the single .csv Timeseries files are stored, to read into the master dataframe.
        start_date : TYPE string
            To create the master dataframe we need to give it a DatetimeIndex that needs a starting point. Set the starting point here.
        end_date : TYPE string
            To create the master dataframe we need to give it a DatetimeIndex that needs an ending point. Set the starting point here.
        amount_to_load : TYPE int
            Define how many of the single .csv Timeseries files should be processed at once. Be careful to not set it too high, as 
            the file size can grow very large very fast.

        Returns
        -------
        df_initial : TYPE pd.DataFrame
            The created master dataframe that merged all the single .csv Timeseries files with all their included columns into one big frame.

        """
        import pandas as pd
        import glob
        #inspect the provided path and load all .csv files in it in a dataframe
        all_files = glob.glob(path_read + "/*.csv")
        
        print(f"There are {len(all_files)} .csv files in the provided path.")
        #adding an error list in which we will save tickers where we got an error loading the data
        error_list = []
        #Set up the initial dataframe which has the longest possible daterange 
        #then based on this initial dataset merge every single file based on the corresponding date
        dates = pd.date_range(start_date,end=end_date).tolist()
        
        df_initial = pd.DataFrame(data = dates,columns=["date"])
        
        #Transforming the entries of Datecolumn to fit the entries of the Date columns of the single files
        for i in range(0,df_initial.shape[0]):
            df_initial.iloc[i,0] = str(df_initial.iloc[i,0])[0:10] 
        #Loading in all single files
        process = pd.DataFrame(all_files,columns=["File"])
        
        for file in all_files[:amount_to_load]:
            df_ = pd.read_csv(file)
            column_indices = [1,2,3,4,5,6]
            new_names = df_.columns[1:7]+"_"+file.split(sep="/")[len( file.split(sep="/"))-1].split(sep=".")[0]
            old_names = df_.columns[column_indices]
            df_.rename(columns=dict(zip(old_names, new_names)), inplace=True)
            #Now all columns are specified with ticker name 
            #Now all the columns with their unique name should be merged with the master dataframe
            df_master = df_initial.merge(right=df_,how="left",on="date")
            df_initial = df_master
            print("The process is loading:",round(process.index[process.File==file][0]/len(all_files[:amount_to_load])*100,3),"%")
            
        return df_initial  
    
    
    
    
    

t = LoadEikonData.CreateMasterDF(path_read = r"/Users/Robert_Hennings/Library/CloudStorage/OneDrive-stud.uni-duisburg-essen.de/BA/Gesamtmarkt USA/full_history",
                   start_date = "1900/1/1",
                   end_date = "2022/1/1",
                   amount_to_load = 1000)
    
#all that is left to do is to combine all the single files to one big dataframe in single periods of 1000s
#first 1000 tickers are loaded and saved into a file
t.to_pickle("/Users/Robert_Hennings/Dokumente/Uni/Bachelor/Bachelorarbeit/FirstThousandTickers.pkl")
# amount_to_load was set to 1000